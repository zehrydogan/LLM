# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkpIxO3os1Mw6gZhBiahOhIy7oKJG32v
"""

import requests
import pandas as pd
from itertools import compress
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import random_split, DataLoader, RandomSampler, SequentialSampler
import random
import time
import datetime
import torch
from torch.utils.data import Dataset
from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW
tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='', eos_token='', pad_token='')
bs = 8
epochs = 3
warmup_steps = 1e2
sample_every = 100
torch.manual_seed(42)
class NewsDataset(Dataset):
    def __init__(self, txt_list, tokenizer, max_length, gpt2_type="gpt2"):
        self.tokenizer = tokenizer
        self.input_ids = []
        self.attn_masks = []
        for txt in txt_list:
            encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding="max_length")
            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))
    def __len__(self):
        return len(self.input_ids)
    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]
def format_time(elapsed):
    return str(datetime.timedelta(seconds=int(round(elapsed))))
def train_model(data, max_length):
    dataset = NewsDataset(data, tokenizer, max_length=max_length)
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=bs)
    validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=bs)
    configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)
    model = GPT2LMHeadModel.from_pretrained("gpt2", config=configuration)
    model.resize_token_embeddings(len(tokenizer))
    device = torch.device("cuda")
    model.to(device)
    seed_val = 42
    random.seed(seed_val)
    np.random.seed(seed_val)
    torch.manual_seed(seed_val)
    torch.cuda.manual_seed_all(seed_val)
    optimizer = AdamW(model.parameters(), lr=5e-4, eps=1e-8)
    total_steps = len(train_dataloader) * epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)
    total_t0 = time.time()
    model.train()

    for epoch_i in range(epochs):
        print(f'Beginning epoch {epoch_i + 1} of {epochs}')
        t0 = time.time()
        total_train_loss = 0
        for step, batch in enumerate(train_dataloader):
            b_input_ids = batch[0].to(device)
            b_labels = batch[0].to(device)
            b_masks = batch[1].to(device)
            model.zero_grad()
            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks, token_type_ids=None)
            loss = outputs[0]
            batch_loss = loss.item()
            total_train_loss += batch_loss
            if step % sample_every == 0 and not step == 0:
                elapsed = format_time(time.time() - t0)
                print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')
                model.eval()
                sample_outputs = model.generate(
                    bos_token_id=random.randint(1, 30000),
                    do_sample=True,
                    top_k=50,
                    max_length=200,
                    top_p=0.95,
                    num_return_sequences=1
                )
                for i, sample_output in enumerate(sample_outputs):
                    print(f'Example output: {tokenizer.decode(sample_output, skip_special_tokens=True)}')
                model.train()
            loss.backward()
            optimizer.step()
            scheduler.step()
        avg_train_loss = total_train_loss / len(train_dataloader)
        training_time = format_time(time.time() - t0)
        print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')
        t0 = time.time()
        model.eval()
        total_eval_loss = 0
        for batch in validation_dataloader:
            b_input_ids = batch[0].to(device)
            b_labels = batch[0].to(device)
            b_masks = batch[1].to(device)
            with torch.no_grad():
                outputs = model(b_input_ids, attention_mask=b_masks, labels=b_labels)
                loss = outputs[0]
            batch_loss = loss.item()
            total_eval_loss += batch_loss
        avg_val_loss = total_eval_loss / len(validation_dataloader)
        validation_time = format_time(time.time() - t0)
        print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')

    print(f'Total training took {format_time(time.time()-total_t0)}')
    return {'model': model, 'device': device}
def generate_text(model, device):
    model.eval()
    prompt = ""
    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
    generated = generated.to(device)
    sample_outputs = model.generate(
        generated,
        do_sample=True,
        top_k=50,
        max_length=500,
        top_p=0.95,
        num_return_sequences=1
    )
    for i, sample_output in enumerate(sample_outputs):
        print("{}: {}\n\n".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))
def get_dataset(file_path):
    data = pd.read_csv(file_path)
    data = data[data['text'].str.len() <= 1024]
    return data["text"]
data_fake = get_dataset('/content/fake.csv')
max_length_fake = getMaxLength(data_fake)
result_fake = train_model(data_fake, max_length_fake)
generate_text(result_fake['model'], result_fake['device'])
data_true = get_dataset('/content/true.csv')
max_length_true = getMaxLength(data_true)
result_true = train_model(data_true, max_length_true)
generate_text(result_true['model'], result_true['device'])